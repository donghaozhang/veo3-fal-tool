---
description: 
globs: 
alwaysApply: true
---
# AI Content Generation Project Overview

This project provides Python utilities for generating videos, images, and avatars using multiple AI platforms:

## Video Generation
1. **Google Veo API** on Vertex AI (higher resolution, automated setup with permission fixes)
2. **FAL AI Dual Models** (simpler setup, production ready)
   - **MiniMax Hailuo-02** (768p, prompt optimizer)
   - **Kling Video 2.1** (high-quality, CFG scale, negative prompts)

## Avatar Generation
3. **FAL AI Triple-Mode System** (talking avatars with lip-sync)
   - **Text-to-Speech**: 20 voice options with natural speech conversion
   - **Audio-to-Avatar**: Custom audio files for lip-sync animation
   - **Multi-Audio Conversation**: Two-person conversations with sequential speaking

## Text-to-Image Generation
4. **FAL AI Quad Models** (consolidated test structure, direct Python API)
   - **Imagen4** (Google's high-quality model)
   - **Seedream** (Artistic and creative generation)
   - **FLUX Schnell** (Ultra-fast generation)
   - **FLUX Dev** (Balanced speed and quality)

## Text-to-Speech Generation
5. **ElevenLabs TTS Package** (modular architecture, comprehensive features)
   - **3000+ Voice Library**: Popular presets and custom voice cloning
   - **AI Content Pipeline**: OpenRouter AI integration with top 10 models
   - **Advanced Features**: Timing control, multi-speaker dialogue, emotional tags
   - **Professional Package**: Modular design with setup.py and proper imports

## Main Components

### Google Veo Implementation
Located in [veo3_video_generation/](mdc:veo3_video_generation) directory with automated setup and permission fixes.

**Key Components:**
- [veo_video_generation.py](mdc:veo3_video_generation/veo_video_generation.py) - Main Google Veo video generation functions
- [fix_permissions.py](mdc:veo3_video_generation/fix_permissions.py) - **Automated permission fix tool** (fixes 90% of setup issues)
- [demo.py](mdc:veo3_video_generation/demo.py) - Interactive demonstration with Veo 2.0/3.0 selection
- [test_veo.py](mdc:veo3_video_generation/test_veo.py) - Comprehensive test suite with command-line options

**Key Functions:**
- `generate_video_from_text()` - Creates videos from text prompts
- `generate_video_from_image()` - Creates videos from images with optional text guidance
- `generate_video_from_local_image()` - Handles local image uploads to GCS
- `generate_video_with_veo3_preview()` - Uses the newer Veo 3.0 model
- `download_gcs_file()` - Downloads generated videos from Google Cloud Storage

**Quick Setup**: Run `python fix_permissions.py` to automatically configure Google Cloud permissions

### FAL AI Video Generation (Dual-Model)
Located in [fal_video_generation/](mdc:fal_video_generation) directory with simplified API-based approach supporting two models and cost-conscious testing.

**Key Components:**
- [fal_video_generator.py](mdc:fal_video_generation/fal_video_generator.py) - Main FALVideoGenerator class with full endpoint names
- [demo.py](mdc:fal_video_generation/demo.py) - Cost-conscious interactive demo with confirmation prompts
- [test_fal_ai.py](mdc:fal_video_generation/test_fal_ai.py) - Cost-conscious test suite with model-specific flags
- [test_api_only.py](mdc:fal_video_generation/test_api_only.py) - **FREE API connection test** (no video generation)
- [README.md](mdc:fal_video_generation/README.md) - Complete FAL AI dual-model documentation
- [COST_CONSCIOUS_TESTING.md](mdc:fal_video_generation/COST_CONSCIOUS_TESTING.md) - Cost-conscious testing guide

### FAL AI Avatar Generation (Triple-Mode)
Located in [fal_avatar_generation/](mdc:fal_avatar_generation) directory with comprehensive avatar video generation using official FAL AI examples.

**Key Components:**
- [fal_avatar_generator.py](mdc:fal_avatar_generation/fal_avatar_generator.py) - Main FALAvatarGenerator class with triple-mode support
- [demo.py](mdc:fal_avatar_generation/demo.py) - Cost-conscious interactive demo with mode selection
- [test_official_example.py](mdc:fal_avatar_generation/test_official_example.py) - **Test using exact FAL AI documentation examples**
- [test_setup.py](mdc:fal_avatar_generation/test_setup.py) - **FREE environment and API validation**
- [test_generation.py](mdc:fal_avatar_generation/test_generation.py) - **PAID avatar generation tests** (includes `--voice`, `--audio`, `--multi` flags)
- [README.md](mdc:fal_avatar_generation/README.md) - Complete FAL AI avatar documentation

### FAL AI Text-to-Image Generation (Quad-Model)
Located in [fal_text_to_image/](mdc:fal_text_to_image) directory with consolidated test structure and direct Python API.

**Key Components:**
- [fal_text_to_image_generator.py](mdc:fal_text_to_image/fal_text_to_image_generator.py) - Main FALTextToImageGenerator class
- [demo.py](mdc:fal_text_to_image/demo.py) - Cost-conscious interactive demo
- **Consolidated Test Suite:**
  - [test_setup.py](mdc:fal_text_to_image/test_setup.py) - **FREE environment and API validation**
  - [test_generation.py](mdc:fal_text_to_image/test_generation.py) - **PAID image generation tests** (includes `--dragon` flag)
- [README.md](mdc:fal_text_to_image/README.md) - Complete FAL AI text-to-image documentation

### ElevenLabs Text-to-Speech Package (Modular Architecture)
Located in [text_to_speech/](mdc:text_to_speech) directory with comprehensive TTS capabilities and AI content pipeline.

**Key Components:**
- [__init__.py](mdc:text_to_speech/__init__.py) - Main package interface with clean imports
- **Core TTS System:**
  - [tts/controller.py](mdc:text_to_speech/tts/controller.py) - Main TTS controller class
  - [tts/voice_manager.py](mdc:text_to_speech/tts/voice_manager.py) - Voice selection and management (3000+ voices)
  - [tts/audio_processor.py](mdc:text_to_speech/tts/audio_processor.py) - Audio format handling and processing
- **AI Content Pipeline:**
  - [pipeline/core.py](mdc:text_to_speech/pipeline/core.py) - OpenRouter AI integration with top 10 models
- **Configuration System:**
  - [config/voices.py](mdc:text_to_speech/config/voices.py) - Voice presets and configurations
  - [config/models.py](mdc:text_to_speech/config/models.py) - AI model settings and recommendations
  - [config/defaults.py](mdc:text_to_speech/config/defaults.py) - Default values and settings
- **CLI Tools:**
  - [cli/interactive.py](mdc:text_to_speech/cli/interactive.py) - Interactive pipeline interface
  - [cli/quick_start.py](mdc:text_to_speech/cli/quick_start.py) - Quick demo runner
- [README.md](mdc:text_to_speech/README.md) - Complete package documentation
- [MIGRATION_GUIDE.md](mdc:text_to_speech/MIGRATION_GUIDE.md) - Migration from old monolithic structure

**Key Features:**
- **3000+ Voice Library**: Popular presets (Rachel, Drew, Bella) and custom voice cloning
- **AI Content Pipeline**: Generate content using Claude Sonnet 4, Gemini 2.0 Flash, DeepSeek V3, etc.
- **Advanced Voice Control**: Timing, speed control, pause insertion, emotional context tags
- **Multi-Speaker Dialogue**: Natural conversation generation with multiple voices
- **Professional Package**: Modular architecture with setup.py, proper imports, and comprehensive testing

**Supported Video Models:**
- **MiniMax Hailuo-02**: `fal-ai/minimax/hailuo-02/standard/image-to-video`
  - Resolution: 768p
  - Duration: 6-10 seconds
  - Features: Prompt optimizer
- **Kling Video 2.1**: `fal-ai/kling-video/v2.1/standard/image-to-video`
  - Resolution: High-quality
  - Duration: 5-10 seconds
  - Features: CFG scale, negative prompts

**Supported Avatar Models:**
- **FAL AI Avatar Single-Text**: `fal-ai/ai-avatar/single-text`
  - Features: 20 voice options, text-to-speech conversion, natural lip-sync
  - Frame Range: 81-129 frames (default: 136)
  - Official Example: Bill voice with podcast-style prompt
- **FAL AI Avatar Audio**: `fal-ai/ai-avatar`
  - Features: Custom audio lip-sync, natural expressions
  - Frame Range: 81-129 frames (default: 145)
  - Supports: MP3, WAV, and other audio formats
- **FAL AI Avatar Multi**: `fal-ai/ai-avatar/multi`
  - Features: Two-person conversations, sequential speaking
  - Frame Range: 81-129 frames (default: 181)
  - Supports: Multiple audio files for conversation flow

**Supported Text-to-Image Models:**
- **Imagen4**: `fal-ai/google/imagen4/text-to-image` - Photorealistic, ~6-8s, $0.015/image
- **Seedream**: `fal-ai/seedream/text-to-image` - Artistic style, ~9-15s, $0.015/image
- **FLUX Schnell**: `fal-ai/flux/schnell` - Ultra-fast, ~1-2s, $0.015/image
- **FLUX Dev**: `fal-ai/flux/dev` - Balanced quality, ~2-3s, $0.015/image

## Project Structure
```
veo3/
├── README.md                        # Multi-platform project overview
├── requirements.txt                 # Main dependencies
├── .env                            # Legacy configuration file
├── archive/                         # Legacy tools and older versions
├── veo3_video_generation/           # Google Veo implementation
│   ├── veo_video_generation.py     # Main Google Veo functions
│   ├── fix_permissions.py          # 🔧 Automated permission fix tool
│   ├── demo.py                     # Interactive Veo demo (2.0/3.0)
│   ├── test_veo.py                 # Comprehensive test suite
│   ├── README.md                   # Veo-specific documentation
│   ├── requirements.txt            # Veo dependencies
│   ├── .env                        # Veo configuration
│   ├── images/                     # Input images (smiling_woman.jpg, bet.png)
│   └── result_folder/              # Veo output videos
├── fal_video_generation/            # FAL AI dual-model video implementation
│   ├── fal_video_generator.py       # FAL AI video generator class (dual-model)
│   ├── demo.py                      # Cost-conscious interactive demo
│   ├── test_fal_ai.py              # Cost-conscious test suite (both models)
│   ├── test_api_only.py            # FREE API connection test
│   ├── README.md                    # FAL AI dual-model documentation
│   ├── COST_CONSCIOUS_TESTING.md   # Cost-conscious testing guide
│   ├── requirements.txt             # FAL AI dependencies
│   ├── .env                         # FAL AI configuration
│   ├── output/                      # FAL AI generated videos
│   └── test_output/                 # Test-generated videos
├── fal_avatar_generation/           # FAL AI triple-mode avatar implementation
│   ├── fal_avatar_generator.py      # FAL AI avatar generator class (triple-mode)
│   ├── demo.py                      # Cost-conscious interactive demo with mode selection
│   ├── test_official_example.py    # Test using exact FAL AI documentation examples
│   ├── test_setup.py               # FREE environment and API validation
│   ├── test_generation.py          # PAID avatar generation tests (voice/audio/multi flags)
│   ├── README.md                    # FAL AI avatar documentation
│   ├── requirements.txt             # FAL AI dependencies
│   ├── .env                         # FAL AI configuration
│   ├── output/                      # Generated avatar videos
│   └── test_output/                 # Test-generated avatar videos
├── fal_text_to_image/               # FAL AI quad-model text-to-image implementation
│   ├── fal_text_to_image_generator.py # FAL AI text-to-image generator class
│   ├── demo.py                      # Cost-conscious interactive demo
│   ├── test_setup.py               # FREE environment and API validation
│   ├── test_generation.py          # PAID image generation tests (includes --dragon)
│   ├── README.md                    # FAL AI text-to-image documentation
│   ├── requirements.txt             # FAL AI dependencies
│   ├── .env                         # FAL AI configuration
│   ├── .gitignore                   # Ignore generated images and outputs
│   ├── output/                      # Generated images
│   └── test_output/                 # Test-generated images
├── text_to_speech/                  # ElevenLabs TTS package (modular architecture)
│   ├── __init__.py                  # Main package interface
│   ├── setup.py                     # Professional package setup
│   ├── README.md                    # Complete package documentation
│   ├── MIGRATION_GUIDE.md          # Migration from old monolithic structure
│   ├── requirements.txt             # TTS dependencies
│   ├── .env                         # ElevenLabs configuration
│   ├── .gitignore                   # Ignore generated audio and cache
│   ├── activate_env.sh             # Environment activation script
│   ├── models/                      # Data models and enums
│   │   ├── common.py               # Shared models (VoiceSettings, AudioFormat)
│   │   └── pipeline.py             # Pipeline-specific models
│   ├── tts/                         # Core TTS functionality
│   │   ├── controller.py           # Main TTS controller class
│   │   ├── voice_manager.py        # Voice selection and management (3000+)
│   │   └── audio_processor.py      # Audio format handling
│   ├── pipeline/                    # OpenRouter AI integration
│   │   └── core.py                 # Complete pipeline orchestration
│   ├── config/                      # Configuration management
│   │   ├── voices.py               # Voice presets and configurations
│   │   ├── models.py               # AI model settings and recommendations
│   │   └── defaults.py             # Default values and settings
│   ├── utils/                       # Utility functions
│   │   ├── file_manager.py         # File operations
│   │   ├── api_helpers.py          # API utilities
│   │   └── validators.py           # Input validation
│   ├── cli/                         # Command line tools
│   │   ├── interactive.py          # Interactive pipeline interface
│   │   └── quick_start.py          # Quick demo runner
│   ├── examples/                    # Usage examples
│   │   └── basic_usage.py          # Basic TTS examples
│   ├── dialogue/                    # Multi-speaker dialogue features
│   └── output/                      # Generated audio files
└── video_tools/                     # Video processing utilities
    ├── video_audio_utils.py         # Video/audio processing functions
    ├── README.md                    # Video tools documentation
    ├── .gitignore                   # Ignore generated videos and audio
    └── sample_video.*               # Sample files for testing
```

## Configuration Requirements

### Google Veo Setup
- **Quick Setup**: Run `python fix_permissions.py` (fixes 90% of issues automatically)
- Google Cloud Project ID (configured in [veo3_video_generation/.env](mdc:veo3_video_generation/.env))
- Google Cloud Storage bucket for output
- Vertex AI API enabled (automated by fix script)
- gcloud CLI authentication

### FAL AI Setup (Simpler)
- FAL AI API key in respective `.env` files:
  - [fal_video_generation/.env](mdc:fal_video_generation/.env)
  - [fal_avatar_generation/.env](mdc:fal_avatar_generation/.env)  
  - [fal_text_to_image/.env](mdc:fal_text_to_image/.env)
- Python dependencies: fal-client, requests, python-dotenv
- Single API key works for all FAL AI models (video, avatar, image)

### ElevenLabs TTS Setup (Professional Package)
- ElevenLabs API key in [text_to_speech/.env](mdc:text_to_speech/.env)
- Optional: OpenRouter API key for AI content pipeline
- Package installation: `pip install -r text_to_speech/requirements.txt`
- Environment activation: `source text_to_speech/activate_env.sh`
- Professional package structure with setup.py and modular imports

## Model Selection Guidelines

### Choose MiniMax Hailuo-02 when:
- You need reliable 768p video generation
- You want AI-powered prompt optimization
- You prefer the established model with proven results

### Choose Kling Video 2.1 when:
- You need high-quality video output
- You want fine control with CFG scale parameters
- You need negative prompt capabilities for better quality control

### Choose Google Veo when:
- You need 1080p resolution
- You want longer videos
- You already use Google Cloud infrastructure

### Choose FAL AI Avatar Generation when:
- You need talking avatars with lip-sync
- You want text-to-speech conversion with 20 voice options
- You need custom audio file lip-sync animation
- You want multi-person conversation videos
- You need natural facial expressions and movements

### Choose ElevenLabs TTS when:
- You need high-quality text-to-speech with 3000+ voices
- You want AI-generated content with automatic speech conversion
- You need multi-speaker dialogue and conversation generation
- You want advanced voice control (timing, emotional tags, custom cloning)
- You need a complete AI content pipeline from text generation to audio

## Usage Recommendations

### Video Generation
- **Testing Setup**: Use `python test_api_only.py` for FREE API validation
- **Prototyping**: Start with FAL AI single model testing (`--hailuo` or `--kling`) to avoid unnecessary costs
- **Production**: Use FAL AI for API-based deployments, Google Veo for high-resolution needs
- **Comparison**: Use cost-conscious comparison tools - remember comparison tests generate 2 videos (~$0.04-0.10)

### Avatar Generation
- **Testing Setup**: Use `python test_setup.py` for FREE environment validation
- **Official Testing**: Use `python test_official_example.py` for exact FAL AI documentation examples
- **Prototyping**: Start with single mode testing (`python test_generation.py --voice Bill`) to avoid unnecessary costs
- **Voice Testing**: Use `python test_generation.py --voice [VoiceName]` for specific voice testing (~$0.03-0.05)
- **Audio Testing**: Use `python test_generation.py --audio` for custom audio file testing (~$0.03-0.05)
- **Conversation Testing**: Use `python test_generation.py --multi` for multi-person conversation testing (~$0.03-0.05)
- **Production**: Use official examples as defaults, customize parameters as needed

### Text-to-Image Generation
- **Testing Setup**: Use `python test_setup.py` for FREE environment validation
- **Prototyping**: Start with single model testing (`python test_generation.py --flux-schnell`) to avoid unnecessary costs
- **Dragon Generation**: Use `python test_generation.py --dragon` for dragon image testing (~$0.015)
- **Production**: Use batch generation for efficiency (`python test_generation.py --batch 1,3`)
- **Comparison**: Use comparison tools carefully - remember comparison tests generate 4 images (~$0.060)

### Text-to-Speech Generation
- **Package Testing**: Use `python -c "from text_to_speech import ElevenLabsTTSController; print('✅ Package working!')"` for FREE import validation
- **Quick Start**: Use `python cli/quick_start.py` for interactive demos and basic testing
- **Interactive Pipeline**: Use `python cli/interactive.py` for full AI content generation pipeline
- **Basic TTS**: Use `python examples/basic_usage.py` for simple text-to-speech examples
- **Voice Testing**: Test specific voices before production use with dummy API keys for structure validation
- **AI Pipeline**: Combine OpenRouter AI content generation with TTS for complete automation
- **Production**: Use modular imports (`from text_to_speech import ElevenLabsTTSController`) for clean integration

## ⚠️ CRITICAL: Cost Protection Rules
**See [cost-protection.mdc](mdc:.cursor/rules/cost-protection.mdc) for complete cost protection guidelines**

### Video Generation
- **NEVER run video generation tests without explicit user confirmation**
- **Always start with FREE tests** (`test_api_only.py`) to validate setup
- **Use model-specific flags** (`--hailuo`, `--kling`) to test individual models
- **Avoid comparison tests** during development unless specifically needed
- **Monitor costs** - each video generation costs ~$0.02-0.05

### Avatar Generation
- **NEVER run avatar generation tests without explicit user confirmation**
- **Always start with FREE tests** (`test_setup.py`) to validate setup
- **Use official examples** (`test_official_example.py`) for documentation compliance
- **Use mode-specific flags** (`--voice`, `--audio`, `--multi`) to test individual modes
- **Avoid comparison tests** during development unless specifically needed
- **Monitor costs** - each avatar generation costs ~$0.02-0.05

### Text-to-Image Generation
- **NEVER run image generation tests without explicit user confirmation**
- **Always start with FREE tests** (`test_setup.py`) to validate setup
- **Use model-specific flags** (`--imagen4`, `--seedream`, `--flux-schnell`, `--flux-dev`) to test individual models
- **Use dragon generation** (`--dragon`) for testing specific scenarios
- **Avoid comparison tests** (`--compare`) during development unless specifically needed
- **Monitor costs** - each image generation costs ~$0.015

### Text-to-Speech Generation
- **Always validate package structure first** with FREE import tests before API calls
- **Use dummy API keys** for development and structure testing
- **Monitor ElevenLabs costs** - speech generation costs vary by character count and voice quality
- **OpenRouter AI costs** - AI content generation costs vary by model (Claude Sonnet 4, Gemini 2.0 Flash, etc.)
- **Start with basic examples** before using advanced AI pipeline features
- **Test voice selection** with short text samples before long-form content









